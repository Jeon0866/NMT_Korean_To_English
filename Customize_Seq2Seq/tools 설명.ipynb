{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "from torch.utils.data import DataLoader\n",
    "from data_helper import create_or_get_voca, LSTMSeq2SeqDataset\n",
    "from Customize_Seq2SeqWithAttention.model import Encoder, AttentionDecoder, Seq2SeqWithAttention\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "# Attention 이미지 만들 때 글씨체 변경\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/NanumBarunGothic.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 먼저 NanumBarunGothic.ttf 를 다운받아서 설치해야 합니다. (https://vhrms.tistory.com/403 저는 여기서 다운받았습니다.)\n",
    "- 만약 설치하지 않으면 Attention 이미지에서 한글이 깨지게 됩니다.\n",
    "- Device는 gpu를 기본으로 하고 없을경우 cpu를 사용하게 되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):  # Train\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.x_train_path = os.path.join(self.args.data_path, self.args.src_train_filename)  # train input 경로 \n",
    "        self.y_train_path = os.path.join(self.args.data_path, self.args.tar_train_filename)  # train target 경로\n",
    "        self.x_val_path = os.path.join(self.args.data_path, self.args.src_val_filename)      # validation input 경로 \n",
    "        self.y_val_path = os.path.join(self.args.data_path, self.args.tar_val_filename)      # validation target 경로\n",
    "        self.ko_voc, self.en_voc = self.get_voca()      # vocabulary\n",
    "        self.train_loader = self.get_train_loader()     # train data loader\n",
    "        self.val_loader = self.get_val_loader()         # validation data loader\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.en_voc['<pad>'])             # cross entropy\n",
    "        self.writer = SummaryWriter()                   # tensorboard 기록\n",
    "        self.train()                                    # train 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self):\n",
    "        start = time.time()\n",
    "        encoder_parameter = self.encoder_parameter()\n",
    "        decoder_parameter = self.decoder_parameter()\n",
    "\n",
    "        encoder = Encoder(**encoder_parameter)\n",
    "        decoder = AttentionDecoder(**decoder_parameter)\n",
    "        model = Seq2SeqWithAttention(encoder, decoder, self.args.sequence_size, self.args.get_attention)\n",
    "        model = nn.DataParallel(model)\n",
    "        model.cuda()\n",
    "        model.train()\n",
    "\n",
    "        encoder_optimizer = opt.Adam(model.parameters(), lr=self.args.learning_rate)\n",
    "        decoder_optimizer = opt.Adam(model.parameters(), lr=self.args.learning_rate)\n",
    "\n",
    "        epoch_step = len(self.train_loader) + 1\n",
    "        total_step = self.args.epochs * epoch_step\n",
    "        train_ratios = cal_teacher_forcing_ratio(self.args.learning_method, total_step)\n",
    "        val_ratios = cal_teacher_forcing_ratio('Mixed_Sampling', int(total_step / 100)+1)\n",
    "\n",
    "        step = 0\n",
    "        attention = None\n",
    "\n",
    "        for epoch in range(self.args.epochs):\n",
    "            for i, data in enumerate(self.train_loader, 0):\n",
    "                try:\n",
    "                    src_input, tar_input, tar_output = data\n",
    "                    if self.args.get_attention:\n",
    "                        output, attention = model(src_input, tar_input, teacher_forcing_rate=train_ratios[i])\n",
    "                    else:\n",
    "                        output = model(src_input, tar_input, teacher_forcing_rate=train_ratios[i])\n",
    "                    # Get loss & accuracy\n",
    "                    loss, accuracy, ppl = self.loss_accuracy(output, tar_output)\n",
    "\n",
    "                    # Training Log\n",
    "                    if step % self.args.train_step_print == 0:\n",
    "                        self.writer.add_scalar('train/loss', loss.item(), step)\n",
    "                        self.writer.add_scalar('train/accuracy', accuracy.item(), step)\n",
    "                        self.writer.add_scalar('train/PPL', ppl, step)\n",
    "\n",
    "                        print('[Train] epoch : {0:2d}  iter: {1:4d}/{2:4d}  step : {3:6d}/{4:6d}  '\n",
    "                              '=>  loss : {5:10f}  accuracy : {6:12f}  PPL : {7:6f}'\n",
    "                              .format(epoch, i, epoch_step, step, total_step, loss.item(), accuracy.item(), ppl))\n",
    "\n",
    "                    # Validation Log\n",
    "                    if step % self.args.val_step_print == 0:\n",
    "                        with torch.no_grad():\n",
    "                            model.eval()\n",
    "                            if step >= 100:\n",
    "                                steps = int(step / 100)\n",
    "                            else:\n",
    "                                steps = step\n",
    "                            val_loss, val_accuracy, val_ppl = self.val(model,\n",
    "                                                                       teacher_forcing_rate=val_ratios[steps])\n",
    "                            self.writer.add_scalar('val/loss', val_loss, step)\n",
    "                            self.writer.add_scalar('val/accuracy', val_accuracy, step)\n",
    "                            self.writer.add_scalar('val/PPL', val_ppl, step)\n",
    "\n",
    "                            print('[Val] epoch : {0:2d}  iter: {1:4d}/{2:4d}  step : {3:6d}/{4:6d}  '\n",
    "                                  '=>  loss : {5:10f}  accuracy : {6:12f}   PPL : {7:10f}'\n",
    "                                  .format(epoch, i, epoch_step, step, total_step, val_loss, val_accuracy, val_ppl))\n",
    "                            model.train()\n",
    "\n",
    "                    # Save Model Point\n",
    "                    if step % self.args.step_save == 0:\n",
    "                        print(\"time :\", time.time() - start)\n",
    "                        if self.args.get_attention:\n",
    "                            self.plot_attention(step, src_input, tar_input, attention)\n",
    "                        self.model_save(model=model, encoder_optimizer=encoder_optimizer,\n",
    "                                        decoder_optimizer=decoder_optimizer, epoch=epoch, step=step)\n",
    "\n",
    "                    # optimizer\n",
    "                    encoder_optimizer.zero_grad()\n",
    "                    decoder_optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    encoder_optimizer.step()\n",
    "                    decoder_optimizer.step()\n",
    "                    step += 1\n",
    "\n",
    "                # If KeyBoard Interrupt Save Model\n",
    "                except KeyboardInterrupt:\n",
    "                    self.model_save(model=model, encoder_optimizer=encoder_optimizer,\n",
    "                                    decoder_optimizer=decoder_optimizer, epoch=epoch, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_teacher_forcing_ratio(learning_method, total_step):\n",
    "    if learning_method == 'Teacher_Forcing':\n",
    "        teacher_forcing_ratios = [1.0 for _ in range(total_step)]  # 교사강요\n",
    "    elif learning_method == 'Scheduled_Sampling':\n",
    "        import numpy as np\n",
    "        teacher_forcing_ratios = np.linspace(0.0, 1.0, num=total_step)[::-1]  # 스케줄 샘플링\n",
    "        # np.linspace : 시작점과 끝점을 균일하게 toptal_step수 만큼 나눈 점을 생성\n",
    "    elif learning_method == 'Mixed_Sampling':\n",
    "        import numpy as np\n",
    "        teacher_forcing_ratios = [1.0 for _ in range(int(total_step/2))]  # 교사강요\n",
    "        b = np.linspace(0.0, 1.0, num=int(total_step/2))[::-1]  # 스케줄 샘플링\n",
    "        teacher_forcing_ratios.extend(b)\n",
    "    else:\n",
    "        raise NotImplementedError('learning method must choice [Teacher_Forcing, Scheduled_Sampling]')\n",
    "    return teacher_forcing_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_voca(self):\n",
    "        try:\n",
    "            ko_voc, en_voc = create_or_get_voca(save_path=self.args.dictionary_path)\n",
    "        except OSError:\n",
    "            ko_voc, en_voc = create_or_get_voca(save_path=self.args.dictionary_path,\n",
    "                                                ko_corpus_path=self.x_train_path,\n",
    "                                                en_corpus_path=self.y_train_path)\n",
    "        return ko_voc, en_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_train_loader(self):\n",
    "        # 재현을 위해 랜덤시드 고정\n",
    "        # seed_val = 42\n",
    "        # torch.manual_seed(seed_val)\n",
    "        # path를 불러와서 train_loader를 만드는 함수\n",
    "        train_dataset = LSTMSeq2SeqDataset(self.x_train_path, self.y_train_path, self.ko_voc, self.en_voc,\n",
    "                                           self.args.sequence_size)\n",
    "        point_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.args.batch_size, sampler=point_sampler)\n",
    "        return train_loader\n",
    "    def get_val_loader(self):\n",
    "        # 재현을 위해 랜덤시드 고정\n",
    "        # seed_val = 42\n",
    "        # torch.manual_seed(seed_val)\n",
    "        # path를 불러와서 train_loader를 만드는 함수\n",
    "        val_dataset = LSTMSeq2SeqDataset(self.x_val_path, self.y_val_path, self.ko_voc, self.en_voc,\n",
    "                                         self.args.sequence_size)\n",
    "        point_sampler = torch.utils.data.RandomSampler(val_dataset)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.args.batch_size, sampler=point_sampler)\n",
    "        return val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def encoder_parameter(self):\n",
    "        param = {\n",
    "            'embedding_size': 5000,\n",
    "            'embedding_dim': self.args.embedding_dim,\n",
    "            'pad_id': self.ko_voc['<pad>'],\n",
    "            'rnn_dim': self.args.encoder_rnn_dim,\n",
    "            'rnn_bias': True,\n",
    "            'n_layers': self.args.encoder_n_layers,\n",
    "            'embedding_dropout': self.args.encoder_embedding_dropout,\n",
    "            'rnn_dropout': self.args.encoder_rnn_dropout,\n",
    "            'dropout': self.args.encoder_dropout,\n",
    "            'residual_used': self.args.encoder_residual_used,\n",
    "            'bidirectional': self.args.encoder_bidirectional_used,\n",
    "            'encoder_output_transformer': self.args.encoder_output_transformer,\n",
    "            'encoder_output_transformer_bias': self.args.encoder_output_transformer_bias,\n",
    "            'encoder_hidden_transformer': self.args.encoder_hidden_transformer,\n",
    "            'encoder_hidden_transformer_bias': self.args.encoder_hidden_transformer_bias\n",
    "        }\n",
    "        return param\n",
    "\n",
    "    def decoder_parameter(self):\n",
    "        param = {\n",
    "            'embedding_size': 5000,\n",
    "            'embedding_dim': self.args.embedding_dim,\n",
    "            'pad_id': self.en_voc['<pad>'],\n",
    "            'rnn_dim': self.args.decoder_rnn_dim,\n",
    "            'rnn_bias': True,\n",
    "            'n_layers': self.args.decoder_n_layers,\n",
    "            'embedding_dropout': self.args.decoder_embedding_dropout,\n",
    "            'rnn_dropout': self.args.decoder_rnn_dropout,\n",
    "            'dropout': self.args.decoder_dropout,\n",
    "            'residual_used': self.args.decoder_residual_used,\n",
    "            'attention_score_func': self.args.attention_score\n",
    "        }\n",
    "        return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def loss_accuracy(self, out, tar):\n",
    "        # out => [embedding_size, sequence_len, vocab_size]\n",
    "        # tar => [embedding_size, sequence_len]\n",
    "        out = out.view(-1, out.size(-1))\n",
    "        tar = tar.view(-1).to(device)\n",
    "        # out => [embedding_size * sequence_len, vocab_size]\n",
    "        # tar => [embedding_size * sequence_len]\n",
    "        loss = self.criterion(out, tar)\n",
    "        ppl = math.exp(loss.item())\n",
    "\n",
    "        _, indices = out.max(-1)\n",
    "        invalid_targets = tar.eq(self.en_voc['<pad>'])\n",
    "        equal = indices.eq(tar)\n",
    "        total = 1\n",
    "        for i in equal.size():\n",
    "            total *= i\n",
    "        accuracy = torch.div(equal.masked_fill_(invalid_targets, 0).long().sum().to(dtype=torch.float32), total)\n",
    "        return loss, accuracy, ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def val(self, model, teacher_forcing_rate):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        total_ppl = 0\n",
    "        with torch.no_grad():\n",
    "            count = 0\n",
    "            for data in self.val_loader:\n",
    "                src_input, tar_input, tar_output = data\n",
    "                output = model(src_input, tar_input, teacher_forcing_rate=teacher_forcing_rate)\n",
    "\n",
    "                if isinstance(output, tuple):\n",
    "                    output = output[0]\n",
    "                loss, accuracy, ppl = self.loss_accuracy(output, tar_output)\n",
    "                total_loss += loss.item()\n",
    "                total_accuracy += accuracy.item()\n",
    "                total_ppl += ppl\n",
    "                count += 1\n",
    "            _, indices = output.view(-1, output.size(-1)).max(-1)\n",
    "            indices = indices[:self.args.sequence_size].tolist()\n",
    "            a = src_input[0].tolist()\n",
    "            b = tar_output[0].tolist()\n",
    "            print(self.tensor2sentence_ko(a))\n",
    "            print(self.tensor2sentence_en(indices))\n",
    "            print(self.tensor2sentence_en(b))\n",
    "            avg_loss = total_loss / count\n",
    "            avg_accuracy = total_accuracy / count\n",
    "            avg_ppl = total_ppl / count\n",
    "            return avg_loss, avg_accuracy, avg_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def model_save(self, model, encoder_optimizer, decoder_optimizer, epoch, step):\n",
    "        model_name = '{0:06d}_model_1.pth'.format(step)\n",
    "        model_path = os.path.join(self.args.model_path, model_name)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'steps': step,\n",
    "            'seq_len': self.args.sequence_size,\n",
    "            'encoder_parameter': self.encoder_parameter(),\n",
    "            'decoder_parameter': self.decoder_parameter(),\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "            'decoder_optimizer_state_dict': decoder_optimizer.state_dict()\n",
    "\n",
    "        }, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def tensor2sentence_en(self, indices: torch.Tensor) -> list:\n",
    "        result = []\n",
    "        translation_sentence = []\n",
    "        for idx in indices:\n",
    "            word = self.en_voc.IdToPiece(idx)\n",
    "            if word == '</s>':\n",
    "                break\n",
    "            translation_sentence.append(word)\n",
    "        translation_sentence = ''.join(translation_sentence).replace('▁', ' ').strip()\n",
    "        result.append(translation_sentence)\n",
    "        return result\n",
    "\n",
    "    def tensor2sentence_ko(self, indices: torch.Tensor) -> list:\n",
    "        result = []\n",
    "        translation_sentence = []\n",
    "        for idx in indices:\n",
    "            word = self.ko_voc.IdToPiece(idx)\n",
    "            if word == '<pad>':\n",
    "                break\n",
    "            translation_sentence.append(word)\n",
    "        translation_sentence = ''.join(translation_sentence).replace('▁', ' ').strip()\n",
    "        result.append(translation_sentence)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def plot_attention(self, step, src_input, trg_input, attention):\n",
    "        filename = '{0:06d}_step'.format(step)\n",
    "        filepath = os.path.join(self.args.img_path, filename)\n",
    "        try:\n",
    "            os.mkdir(filepath)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        def replace_pad(words):\n",
    "            return [word if word != '<pad>' else '' for word in words]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            src_input = src_input.to('cpu')\n",
    "            trg_input = trg_input.to('cpu')\n",
    "            attention = attention.to('cpu')\n",
    "\n",
    "            sample = [i for i in range(src_input.shape[0] - 1)]\n",
    "            sample = random.sample(sample, self.args.plot_count)\n",
    "\n",
    "            for num, i in enumerate(sample):\n",
    "                src, trg = src_input[i], trg_input[i]\n",
    "                src_word = replace_pad([self.ko_voc.IdToPiece(word.item()) for word in src])\n",
    "                trg_word = replace_pad([self.en_voc.IdToPiece(word.item()) for word in trg])\n",
    "\n",
    "                fig = plt.figure()\n",
    "                ax = fig.add_subplot(111)\n",
    "                cax = ax.matshow(attention[i].data, cmap='bone')\n",
    "                fig.colorbar(cax)\n",
    "\n",
    "                ax.set_xticklabels(trg_word, rotation=90)\n",
    "                ax.set_yticklabels(src_word)\n",
    "                ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "                ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "                fig.savefig(fname=os.path.join(filepath, 'attention-{}.png'.format(num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
